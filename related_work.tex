\section{Related Work}
Multiple previous attempts to automatically schedule Halide programs have been proposed. In the original paper~\cite{ragan2013halide}, heuristics and genetic search over random schedule rewrites were used. The scheduling relied on measuring real execution time, which resulted in a search time that can get prolonged to days for moderate benchmarks. OpenTuner~\cite{ansel2014opentuner} autotunes a program using an AUC-Bandit-meta-technique-directed ensemble selection of algorithms. It is effective in scheduling simple pipelines \cite{mullapudi2015polymage}.

The auto-scheduler that comes with the master Halide repo, which we compare against in Section~\ref{sec:res} is based on~\cite{mullapudi2016automatically}. It uses a cost model with a greedy search algorithm that allows it to run quickly, with not autotuning or benchmarking at all. However, it only considers a fixed set of optimization heuristics for things like parallelism, vectorization and unrolling, and a single level of tiling and fusion. \cite{jangda2018effective} improved this cost model, but did not expand the restricted search space. \cite{sioutas2018loop,sioutas2019schedule} improves the search space and uses a manual cost model. However, the search space is still smaller than ours. 

Many compilers that use loop polyhedral analysis to perform automatic scheduling of affine loop nests~\cite{mullapudi2015polymage,vasilache2018tensor,baghdadi2015pencil,bondhugula2008practical,grosser2012polly,baghdadi2019tiramisu}. Many possible Halide schedules are excluded in these compilers. However, it might be possible to use the polyhedral representation in building more accurate cost models, which we plan to do in future work.

AutoTVM~\cite{chen2018learning} uses tree-based algorithms to auto-schedule programs on TVM~\cite{chen2018tvm}, which is an optimization stack for deep learning. This approach however still requires the user to manually write the search space for each loop. Furthermore, each operation is optimized in isolation without exploring large programs. \cite{ahn2019reinforcement} uses deep reinforcement learning to schedule deep learning pipelines and improves the performance compared to AutoTVM.

Machine learning in compiler optimization has been proposed in many prior works~\cite{wang2018machine,ashouri2018survey}. This includes phase ordering~\cite{fursin2008milepost,huang2013effect,huang2015effect,agakov2006using,2003Stephenson,2012Kulkarni}, tiling factors~\cite{rahman2010neural}, mappings of kernels to CPUs or GPUs~\cite{cummins2017end} with supervised learning, auto-vectorization~\cite{haj2020neurovectorizer,stock2012using,tian2016llvm,trifunovic2009polyhedral,nuzman2011vapor,porpodas2015throttling,larsen2000exploiting,mcfarlin2011automatic,porpodas2017supergraph,porpodas2015pslp,mendis2019compiler} and the throughput of basic blocks~\cite{mendis2018ithemal}.

Multi-Level Intermediate Representation (MLIR)~\cite{lattner2020mlir} has been recently proposed to help with scaling the performance with the end of Moore's law. One objective of MLIR is to represent kernels in a form suitable for optimization, and allow easy integration of search algorithms such as reinforcement learning, MCTS, and beam search.



